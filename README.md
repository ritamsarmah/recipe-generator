# Neural Recipe Generation

By: Eric Kong, Ritam Sarmah, Isha Verma

This repository includes Jupyter notebooks which implement the data preprocessing,  training, and evaluation of our recipe generation models. We trained and evaluated two models: a character-level GRU-based model and GPT-2. Our code was tested on Google Colaboratory.

Here are short descriptions of the notebooks:

1. **Preprocessing**

    - `preprocess_rnn.ipynb`: Preprocesses dataset for RNN.
    - `preprocess_pickle.ipynb`: Preprocesses dataset for GPT-2.

2. **Training**

    - `rnn_char.ipynb`: Trains character-level RNN and generates sample recipes for evaluation.
    - `transformer_huggingface.ipynb`: Fine-tunes GPT-2 and generates sample recipes for evaluation.

3. **Evaluation**

    - `automated_metrics.ipynb`: Calculates automated evaluation metrics (BLEU-4 and percentage of correct ingredients) for both models.
    - `discriminator_DistilBERT.ipynb`: Trains DistilBERT to distinguish between ground truth recipes and RNN-produced recipes, as well as between ground truth recipes and GPT-2 fine-tuned recipes. The recipes are generated by selecting a random word from the titles in the dataset as the prompt to the models.
    - `discriminator_lstm.ipynb`: Trains an LSTM to distinguish between ground truth recipes and RNN-produced recipes, as well as between ground truth recipes and GPT-2 fine-tuned recipes. The recipes are generated by selecting a random word from the titles in the dataset as the prompt to the models.
    - `discriminator_lstm_titles.ipynb`: Trains an LSTM to distinguish between ground truth recipes and RNN-produced recipes, as well as between ground truth recipes and GPT-2 fine-tuned recipes. The recipes are generated by selecting a random title in the dataset as the prompt to the models.

4. **Miscellaneous**

    - `pretty_print_recipes.ipynb`: Applies consistent formatting to generated recipes for human evaluation (used for human evaluation studies).

    - `presentation_demo.ipynb`: Generates recipes using the fine-tuned GPT-2 model.

